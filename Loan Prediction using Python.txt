
#Loading Packages
import pandas as pd
import numpy as np                     # For mathematical calculations
import seaborn as sns                  # For data visualization
import matplotlib.pyplot as plt        # For plotting graphs
%matplotlib inline
import warnings                        # To ignore any warnings
warnings.filterwarnings("ignore")

#Reading data
train=pd.read_csv("train_u6lujuX_CVtuZ9i.csv")
test=pd.read_csv("test_Y3wMUE5_7gLdaTN.csv")

#Lets make a copy of train and test data so that even if we have to make any changes 
#in these datasets we would not lose the original datasets.

train_original=train.copy()
test_original=test.copy()

#Understanding the Data
#Let's look at the structure of the train and test datasets. 
#Firstly, we will check the features present in our data and then we will look at their data types.

train.columns
test.columns

# Print data types for each variable
train.dtypes
# Let's look at the shape of the dataset.

train.shape, test.shape
# We have 614 rows and 13 columns in the train dataset and 367 rows and 12 columns in test dataset.

## EDA

# Univariate Analysis
#In this section, we will do univariate analysis. It is the simplest form of analyzing data where we 
#examine each variable individually. For categorical features we can use frequency table or bar plots 
#which will calculate the number of each category in a particular variable. For numerical features, 
#probability density plots can be used to look at the distribution of the variable.
#Target Variable
#We will first look at the target variable, i.e., Loan_Status. As it is a categorical variable, let us 
#look at its frequency table, percentage distribution and bar plot.
#Frequency table of a variable will give us the count of each category in that variable.

train['Loan_Status'].value_counts()

# Normalize can be set to True to print proportions instead of number 

train['Loan_Status'].value_counts(normalize=True)
train['Loan_Status'].value_counts().plot.bar()

#The loan of 422(around 69%) people out of 614 was approved.
#Now lets visualize each variable separately. Different types of variables are 
#Categorical, ordinal and numerical.LetÅfs visualize the categorical and ordinal features first.

#Independent Variable (Categorical)
plt.figure(1)
plt.subplot(221)
train['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender')

plt.subplot(222)
train['Married'].value_counts(normalize=True).plot.bar(title= 'Married')

plt.subplot(223)
train['Self_Employed'].value_counts(normalize=True).plot.bar(title= 'Self_Employed')

plt.subplot(224)
train['Credit_History'].value_counts(normalize=True).plot.bar(title= 'Credit_History')

plt.show()
It can be inferred from the above bar plots that:

#80% applicants in the dataset are male.
#Around 65% of the applicants in the dataset are married.
#Around 15% applicants in the dataset are self employed.
#Around 85% applicants have repaid their debts.
#Now letÅfs visualize the ordinal variables.
#Independent Variable (Ordinal)

plt.figure(1)
plt.subplot(131)
train['Dependents'].value_counts(normalize=True).plot.bar(figsize=(24,6), title= 'Dependents')

plt.subplot(132)
train['Education'].value_counts(normalize=True).plot.bar(title= 'Education')

plt.subplot(133)
train['Property_Area'].value_counts(normalize=True).plot.bar(title= 'Property_Area')

plt.show()

#Following inferences can be made from the above bar plots:

#Most of the applicants donÅft have any dependents.
#Around 80% of the applicants are Graduate.
#Most of the applicants are from Semiurban area.

plt.figure(1)
plt.subplot(121)
sns.distplot(train['ApplicantIncome']);

plt.subplot(122)
train['ApplicantIncome'].plot.box(figsize=(16,5))

plt.show()

#Independent Variable (Numerical)
#Till now we have seen the categorical and ordinal variables and now lets visualize
#the numerical variables. Lets look at the distribution of Applicant income first.
#The boxplot confirms the presence of a lot of outliers/extreme values. This can be 
#attributed to the income disparity in the society. Part of this can be driven by the
#fact that we are looking at people with different education levels. Let us segregate
#them by Education:

train.boxplot(column='ApplicantIncome', by = 'Education')
plt.suptitle("")

#We can see that there are a higher number of graduates with very high incomes, which
#are appearing to be the outliers.

#LetÅfs look at the Coapplicant income distribution.

plt.figure(1)
plt.subplot(121)
sns.distplot(train['CoapplicantIncome']);

plt.subplot(122)
train['CoapplicantIncome'].plot.box(figsize=(16,5))

plt.show()

#We see a similar distribution as that of the applicant income. Majority of coapplicants
#income ranges from 0 to 5000. We also see a lot of outliers in the coapplicant income
#and it is not normally distributed.
#Let's look at the distribution of LoanAmount variable.

plt.figure(1)
plt.subplot(121)
df=train.dropna()
sns.distplot(df['LoanAmount']);

plt.subplot(122)
train['LoanAmount'].plot.box(figsize=(16,5))

plt.show()

#We see a lot of outliers in this variable and the distribution is fairly normal. We 
#will treat the outliers in later sections.
#Now we would like to know how well each feature correlate with Loan Status. 

## Bivariate Analysis
#Lets recall some of the hypotheses that we generated earlier:

# Applicants with high income should have more chances of loan approval.
# Applicants who have repaid their previous debts should have higher chances of loan approval.
# Loan approval should also depend on the loan amount. If the loan amount is less, chances of
# loan approval should be high.
# Lesser the amount to be paid monthly to repay the loan, higher the chances of loan approval.
# Let's try to test the above mentioned hypotheses using bivariate analysis.

# After looking at every variable individually in univariate analysis, we will now explore them again with
# respect to the target variable.

## Categorical Independent Variable vs Target Variable
#First of all we will find the relation between target variable and categorical 
#independent variables. 

Gender=pd.crosstab(train['Gender'],train['Loan_Status'])
Gender.div(Gender.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(4,4))

#It can be inferred that the proportion of male and female applicants is more or less same
#for both approved and unapproved loans.

#Now let us visualize the remaining categorical variables vs target variable.

Married=pd.crosstab(train['Married'],train['Loan_Status'])
Dependents=pd.crosstab(train['Dependents'],train['Loan_Status'])
Education=pd.crosstab(train['Education'],train['Loan_Status'])
Self_Employed=pd.crosstab(train['Self_Employed'],train['Loan_Status'])

Married.div(Married.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(4,4))
plt.show()

Dependents.div(Dependents.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.show()

Education.div(Education.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(4,4))
plt.show()

Self_Employed.div(Self_Employed.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(4,4))
plt.show()

#Proportion of married applicants is higher for the approved loans.
#Distribution of applicants with 1 or 3+ dependents is similar across both the categories of Loan_Status.
#There is nothing significant we can infer from Self_Employed vs Loan_Status plot.
#Now we will look at the relationship between remaining categorical independent variables and Loan_Status.

Credit_History=pd.crosstab(train['Credit_History'],train['Loan_Status'])
Property_Area=pd.crosstab(train['Property_Area'],train['Loan_Status'])

Credit_History.div(Credit_History.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(4,4))
plt.show()

Property_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.show()

#It seems people with credit history as 1 are more likely to get their loans approved.
#Proportion of loans getting approved in semiurban area is higher as compared to that in rural or urban areas.
#Now letÅfs visualize numerical independent variables with respect to target variable.

##Numerical Independent Variable vs Target Variable
#We will try to find the mean income of people for which the loan has been approved vs the mean income 
#of people for which the loan has not been approved.

train.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()

#Here the y-axis represents the mean applicant income. We donÅft see any change in the mean income. 
#So, let's make bins for the applicant income variable based on the values in it and analyze the corresponding
#loan status for each bin.

bins=[0,2500,4000,6000,81000]
group=['Low','Average','High', 'Very high']
train['Income_bin']=pd.cut(df['ApplicantIncome'],bins,labels=group)
Income_bin=pd.crosstab(train['Income_bin'],train['Loan_Status'])
Income_bin.div(Income_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('ApplicantIncome')
P = plt.ylabel('Percentage')

#It can be inferred that Applicant income does not affect the chances of loan approval which contradicts 
#our hypothesis in which we assumed that if the applicant income is high the chances of loan approval will also #be high.

#We will analyze the coapplicant income and loan amount variable in similar manner.

bins=[0,1000,3000,42000]
group=['Low','Average','High']
train['Coapplicant_Income_bin']=pd.cut(df['CoapplicantIncome'],bins,labels=group)
Coapplicant_Income_bin=pd.crosstab(train['Coapplicant_Income_bin'],train['Loan_Status'])
Coapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('CoapplicantIncome')
P = plt.ylabel('Percentage')

#It shows that if coapplicantÅfs income is less the chances of loan approval are high. But this does not 
#look right. The possible reason behind this may be that most of the applicants donÅft have any coapplicant
# so the coapplicant income for such applicants is 0 and hence the loan approval is not dependent on it.
# So we can make a new variable in which we will combine the applicantÅfs and coapplicantÅfs income to 
#visualize the combined effect of income on loan approval.

#Let us combine the Applicant Income and Coapplicant Income and see the combined effect of Total Income on the #Loan_Status.

train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome']
bins=[0,2500,4000,6000,81000]
group=['Low','Average','High', 'Very high']
train['Total_Income_bin']=pd.cut(train['Total_Income'],bins,labels=group)
Total_Income_bin=pd.crosstab(train['Total_Income_bin'],train['Loan_Status'])
Total_Income_bin.div(Total_Income_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('Total_Income')
P = plt.ylabel('Percentage')

#We can see that Proportion of loans getting approved for applicants having low Total_Income is very
#less as compared to that of applicants with Average, High and Very High Income.

#Let's visualize the Loan amount variable.

bins=[0,100,200,700]
group=['Low','Average','High']
train['LoanAmount_bin']=pd.cut(df['LoanAmount'],bins,labels=group)
LoanAmount_bin=pd.crosstab(train['LoanAmount_bin'],train['Loan_Status'])
LoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True)
plt.xlabel('LoanAmount')
P = plt.ylabel('Percentage')

#It can be seen that the proportion of approved loans is higher for Low and Average Loan Amount
# as compared to that of High Loan Amount which supports our hypothesis in which we considered that 
#the chances of loan approval will be high when the loan amount is less.

#Let's drop the bins which we created for the exploration part. We will change the 3+ in dependents variable
# to 3 to make it a numerical variable.We will also convert the target variableÅfs categories into 0 and 1 so
#that we can find its correlation with numerical variables. One more reason to do so is few models like logistic
# regression takes only numeric values as input. We will replace N with 0 and Y with 1.

train=train.drop(['Income_bin', 'Coapplicant_Income_bin', 'LoanAmount_bin', 'Total_Income_bin', 'Total_Income'], axis=1)
train['Dependents'].replace('3+', 3,inplace=True)
test['Dependents'].replace('3+', 3,inplace=True)
train['Loan_Status'].replace('N', 0,inplace=True)
train['Loan_Status'].replace('Y', 1,inplace=True)

#Now lets look at the correlation between all the numerical variables. We will use the heat map to visualize 
#the correlation. Heatmaps visualize data through variations in coloring. The variables with darker color means 
#their correlation is more.

matrix = train.corr()
f, ax = plt.subplots(figsize=(9, 6))
sns.heatmap(matrix, vmax=.8, square=True, cmap="BuPu");

#We see that the most correlated variables are (ApplicantIncome - LoanAmount) and (Credit_History - #Loan_Status). LoanAmount is also correlated with CoapplicantIncome.


## Missing Value and Outlier Treatment
#After exploring all the variables in our data, we can now impute the missing values and treat the
# outliers because missing data and outliers can have adverse effect on the model performance.

#Missing value imputation
#Let's list out feature-wise count of missing values.

train.isnull().sum()
#We will treat the missing values in all the features one by one.
#We can consider these methods to fill the missing values:

#For numerical variables: imputation using mean or median
#For categorical variables: imputation using mode
#There are very less missing values in Gender, Married, Dependents, Credit_History and Self_Employed features 
#so we can fill them using the mode of the features.

train['Gender'].fillna(train['Gender'].mode()[0], inplace=True)
train['Married'].fillna(train['Married'].mode()[0], inplace=True)
train['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True)
train['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True)
train['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True)
#Now let's try to find a way to fill the missing values in Loan_Amount_Term. We will look at the 
#value count of the Loan amount term variable.

train['Loan_Amount_Term'].value_counts()

#It can be seen that in loan amount term variable, the value of 360 is repeating the most. 
#So we will replace the missing values in this variable using the mode of this variable.

train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True)

#Now we will see the LoanAmount variable. As it is a numerical variable, we can use mean or median 
#to impute the missing values. We will use median to fill the null values as earlier we saw that loan
# amount have outliers so the mean will not be the proper approach as it is highly affected by the presence of #outliers.

train['LoanAmount'].fillna(train['LoanAmount'].median(), inplace=True)
#Now lets check whether all the missing values are filled in the dataset.

train.isnull().sum()

#As we can see that all the missing values have been filled in the test dataset. Let's fill all the
# missing values in the test dataset too with the same approach.

test['Gender'].fillna(train['Gender'].mode()[0], inplace=True)
test['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True)
test['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True)
test['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True)
test['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True)
test['LoanAmount'].fillna(train['LoanAmount'].median(), inplace=True)
#Outlier Treatment
#As we saw earlier in univariate analysis, LoanAmount contains outliers so we have to treat them as the 
#presence of outliers affects the distribution of the data. 

train['LoanAmount_log'] = np.log(train['LoanAmount'])
train['LoanAmount_log'].hist(bins=20)
test['LoanAmount_log'] = np.log(test['LoanAmount'])

#Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.
# Let's build a logistic regression model and make predictions for the test dataset.
## Modeling and Feature Engineering  

## Evaluation Metrics for Classification Problems
#At the competition page, it has been mentioned that the submission data would be evaluated based 
#on the accuracy. Hence, we will use accuracy as our evaluation metric.


## Model Building : Part I
#Let us make our first model to predict the target variable. We will start with Logistic Regression which is #used for predicting binary outcome.

#Logistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, True / False)
#given a set of independent variables. Logistic regression is an estimation of Logit function. Logit function 
#is simply a log of odds in favor of the event.
#This function creates a s-shaped curve with the probability estimate, which is very similar to the required 
#step wise function
#Lets drop the Loan_ID variable as it do not have any effect on the loan status. We will do the same changes 
#to the test dataset which we did for the training dataset.

train=train.drop('Loan_ID',axis=1)
test=test.drop('Loan_ID',axis=1)

#We will use scikit-learn (sklearn) for making different models which is an open source library for Python. 
#It is one of the most efficient tool which contains many inbuilt functions that can be used for modeling in #Python.
#Sklearn requires the target variable in a separate dataset. So, we will drop our target variable from the train 
#dataset and save it in another dataset.

X = train.drop('Loan_Status',1)
y = train.Loan_Status

#Now we will make dummy variables for the categorical variables. Dummy variable turns categorical variables 
#into a series of 0 and 1, making them lot easier to quantify and compare. Let us understand the process of dummies first:

#Consider the Gender variable. It has two classes, Male and Female.
#As logistic regression takes only the numerical values as input, we have to change male and female into #numerical value.
#Once we apply dummies to this variable, it will convert the ÅgGenderÅh variable into two variables(Gender_Male #and Gender_Female), one for each class, i.e. Male and Female.
#Gender_Male will have a value of 0 if the gender is Female and a value of 1 if the gender is Male.

X=pd.get_dummies(X)
train=pd.get_dummies(train)
test=pd.get_dummies(test)

#Now we will train the model on training dataset and make predictions for the test dataset. 
#But can we validate these predictions? One way of doing this is we can divide our train dataset into two parts: 
#train and validation. We can train the model on this train part and using that make predictions for the #validation part. 
#In this way we can validate our predictions as we have the true predictions for the validation part 
#(which we do not have for the test dataset).

#We will use the train_test_split function from sklearn to divide our train dataset. 
#So, first let us import train_test_split.

from sklearn.model_selection import train_test_split
x_train, x_cv, y_train, y_cv = train_test_split(X,y, test_size =0.3)

#The dataset has been divided into training and validation part.
#Let us import LogisticRegression and accuracy_score from sklearn and fit the logistic regression model.

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
model = LogisticRegression()
model.fit(x_train, y_train)

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)

#Let's predict the Loan_Status for validation set and calculate its accuracy.

pred_cv = model.predict(x_cv)

#Let us calculate how accurate our predictions are by calculating the accuracy.

accuracy_score(y_cv,pred_cv)
0.7945945945945946

#So our predictions are almost 80% accurate, i.e. we have identified 80% of the loan status correctly.

#Let's make predictions for the test dataset.

pred_test = model.predict(test)

#Lets import the submission file which we have to submit on the solution checker.

submission=pd.read_csv("Sample_Submission_ZAuTl8O_FK3zQHh.csv")

#We only need the Loan_ID and the corresponding Loan_Status for the final submission. 
#We will fill these columns with the Loan_ID of test dataset and the predictions that we made,
#i.e., pred_test respectively.

submission['Loan_Status']=pred_test
submission['Loan_ID']=test_original['Loan_ID']

#Remember we need predictions in Y and N. So convert 1 and 0 to Y and N.

submission['Loan_Status'].replace(0, 'N',inplace=True)
submission['Loan_Status'].replace(1, 'Y',inplace=True)

#Finally we will convert the submission to .csv format and make submission to check the accuracy on the leaderboard.

pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('logistic.csv')

#From this submission we got an accuracy of 0.7847 on the leaderboard.

#Instead of creating validation set, we can also make use of cross validation to validate our predictions.See #later.


## Logistic Regression using stratified k-folds cross validation
#To check how robust our model is to unseen data, we can use Validation. It is a technique 
#which involves reserving a particular sample of a dataset on which you do not train the model. 
#Later, you test your model on this sample before finalizing it. Some of the common methods for 
#validation are listed below:

#The validation set approach
#k-fold cross validation
#Leave one out cross validation (LOOCV)
#Stratified k-fold cross validation

#In this section we will learn about stratified k-fold cross validation. 
#Let's import StratifiedKFold from sklearn and fit the model.

from sklearn.model_selection import StratifiedKFold

#Now let's make a cross validation logistic model with stratified 5 folds and make predictions for test dataset.

i=1
kf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)
for train_index,test_index in kf.split(X,y):
     print('\n{} of kfold {}'.format(i,kf.n_splits))
     xtr,xvl = X.loc[train_index],X.loc[test_index]
     ytr,yvl = y[train_index],y[test_index]
    
     model = LogisticRegression(random_state=1)
     model.fit(xtr, ytr)
     pred_test = model.predict(xvl)
     score = accuracy_score(yvl,pred_test)
     print('accuracy_score',score)
     i+=1
pred_test = model.predict(test)
pred=model.predict_proba(xvl)[:,1]

#The mean validation accuracy for this model turns out to be 0.81. Let us visualize the roc curve.

from sklearn import metrics
fpr, tpr, _ = metrics.roc_curve(yvl,  pred)
auc = metrics.roc_auc_score(yvl, pred)
plt.figure(figsize=(12,8))
plt.plot(fpr,tpr,label="validation, auc="+str(auc))
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc=4)
plt.show()

#We got an auc value of 0.77.

submission['Loan_Status']=pred_test
submission['Loan_ID']=test_original['Loan_ID']
Remember we need predictions in Y and N. So letÅfs convert 1 and 0 to Y and N.

submission['Loan_Status'].replace(0, 'N',inplace=True)
submission['Loan_Status'].replace(1, 'Y',inplace=True)

#Lets convert the submission to .csv format and make submission to check the accuracy on the leaderboard.
pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Logistic.csv')

#From this submission we got an accuracy of 0.78472 on the leaderboard. 
#Now we will try to improve this accuracy using different approaches.

## Feature Engineering

#Based on the domain knowledge, we can come up with new features that might affect the target variable. 
#We will create the following three new features:

#Total Income - As discussed during bivariate analysis we will combine the Applicant Income and Coapplicant #Income. 
#If the total income is high, chances of loan approval might also be high.
#EMI - EMI is the monthly amount to be paid by the applicant to repay the loan.
#Idea behind making this variable is that people who have high EMIÅfs might find it difficult to pay back the loan.
# We can calculate the EMI by taking the ratio of loan amount with respect to loan amount term.
#Balance Income - This is the income left after the EMI has been paid. Idea behind creating this variable is #that 
#if this value is high, the chances are high that a person will repay the loan and hence increasing the chances #of loan approval.
train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome']
test['Total_Income']=test['ApplicantIncome']+test['CoapplicantIncome']

#Let's check the distribution of Total Income.

sns.distplot(train['Total_Income']);

#We can see it is shifted towards left, i.e., the distribution is right skewed. 
#So, let's take the log transformation to make the distribution normal.

train['Total_Income_log'] = np.log(train['Total_Income'])
sns.distplot(train['Total_Income_log']);
test['Total_Income_log'] = np.log(test['Total_Income'])

#Now the distribution looks much closer to normal and effect of extreme values has been 
##significantly subsided.
#Let's create the EMI feature now.

train['EMI']=train['LoanAmount']/train['Loan_Amount_Term']
test['EMI']=test['LoanAmount']/test['Loan_Amount_Term']
#Let's check the distribution of EMI variable.

sns.distplot(train['EMI']);

#Let us create Balance Income feature now and check its distribution.

train['Balance Income']=train['Total_Income']-(train['EMI']*1000) 
# Multiply with 1000 to make the units equal 

test['Balance Income']=test['Total_Income']-(test['EMI']*1000)
sns.distplot(train['Balance Income']);

#Let us now drop the variables which we used to create these new features.
# Reason for doing this is, the correlation between those old features and these new 
#features will be very high #and logistic regression assumes that the variables are not highly correlated. 
#We also wants to remove the noise from the dataset, so removing correlated features will help in reducing the noise too.

train=train.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1)
test=test.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1)

## Model Building : Part II

#After creating new features, we can continue the model building process. 
#So we will start with logistic regression model and then move over to more complex models 
#like RandomForest and XGBoost.

#We will build the following models in this section.

#Logistic Regression
#Decision Tree
#Random Forest
#XGBoost
#LetÅfs prepare the data for feeding into the models.

X = train.drop('Loan_Status',1)
y = train.Loan_Status                # Save target variable in separate dataset

#Logistic Regression
i=1
kf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)
for train_index,test_index in kf.split(X,y):
     print('\n{} of kfold {}'.format(i,kf.n_splits))
     xtr,xvl = X.loc[train_index],X.loc[test_index]
     ytr,yvl = y[train_index],y[test_index]
    
     model = LogisticRegression(random_state=1)
     model.fit(xtr, ytr)
     pred_test = model.predict(xvl)
     score = accuracy_score(yvl,pred_test)
     print('accuracy_score',score)
     i+=1
pred_test = model.predict(test)
pred=model.predict_proba(xvl)[:,1]

#The mean validation accuracy for this model is 0.812

submission['Loan_Status']=pred_test            # filling Loan_Status with predictions
submission['Loan_ID']=test_original['Loan_ID'] # filling Loan_ID with test Loan_ID
# replacing 0 and 1 with N and Y
submission['Loan_Status'].replace(0, 'N',inplace=True)
submission['Loan_Status'].replace(1, 'Y',inplace=True)
# Converting submission file to .csv format
pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Log2.csv')

#From this submission we got an accuracy of 0.7847 on the leaderboard. 
#So we can infer feature engineering has not improved the model. Let us look at some other #algorithms.

##Decision Tree

from sklearn import tree
#Let's fit the decision tree model with 5 folds of cross validation.

i=1
kf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)
for train_index,test_index in kf.split(X,y):
     print('\n{} of kfold {}'.format(i,kf.n_splits))
     xtr,xvl = X.loc[train_index],X.loc[test_index]
     ytr,yvl = y[train_index],y[test_index]
    
     model = tree.DecisionTreeClassifier(random_state=1)
     model.fit(xtr, ytr)
     pred_test = model.predict(xvl)
     score = accuracy_score(yvl,pred_test)
     print('accuracy_score',score)
     i+=1
pred_test = model.predict(test)

#The mean validation accuracy for this model is 0.69

submission['Loan_Status']=pred_test            # filling Loan_Status with predictions
submission['Loan_ID']=test_original['Loan_ID'] # filling Loan_ID with test Loan_ID
# replacing 0 and 1 with N and Y
submission['Loan_Status'].replace(0, 'N',inplace=True)
submission['Loan_Status'].replace(1, 'Y',inplace=True)
# Converting submission file to .csv format
pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Decision Tree.csv')

#We got an accuracy of 0.63 which is much lesser than the accuracy from logistic #regression model. 
#So letÅfs build another model, i.e. Random Forest, a tree based ensemble algorithm and #try to improve our model by improving the accuracy.

#Random Forest
from sklearn.ensemble import RandomForestClassifier
i=1
kf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)
for train_index,test_index in kf.split(X,y):
     print('\n{} of kfold {}'.format(i,kf.n_splits))
     xtr,xvl = X.loc[train_index],X.loc[test_index]
     ytr,yvl = y[train_index],y[test_index]
    
     model = RandomForestClassifier(random_state=1, max_depth=10)
     model.fit(xtr, ytr)
     pred_test = model.predict(xvl)
     score = accuracy_score(yvl,pred_test)
     print('accuracy_score',score)
     i+=1
pred_test = model.predict(test)

#The mean validation accuracy for this model is 0.766

#We will try to improve the accuracy by tuning the hyperparameters for this model. 
#We will use grid search to get the optimized values of hyper parameters.
#Grid-search is a way to select the best of a family of hyper parameters, parametrized by #a grid of parameters.

#We will tune the max_depth and n_estimators parameters. max_depth decides the maximum #depth of the tree and n_estimators decides the number of trees that will be used in #random forest model.

from sklearn.model_selection import GridSearchCV
# Provide range for max_depth from 1 to 20 with an interval of 2 and from 1 to 200 with an #interval of 20 for n_estimators

paramgrid = {'max_depth': list(range(1, 20, 2)), 'n_estimators': list(range(1, 200, 20))}
grid_search=GridSearchCV(RandomForestClassifier(random_state=1),paramgrid)
from sklearn.model_selection import train_test_split
x_train, x_cv, y_train, y_cv = train_test_split(X,y, test_size =0.3, random_state=1)

# Fit the grid search model
grid_search.fit(x_train,y_train)

GridSearchCV(cv=None, error_score='raise',
       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=1, verbose=0, warm_start=False),
       fit_params=None, iid=True, n_jobs=1,
       param_grid={'max_depth': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19], 'n_estimators': [1, 21, 41, 61, 81, 101, 121, 141, 161, 181]},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring=None, verbose=0)
# Estimating the optimized value
grid_search.best_estimator_

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=3, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=41, n_jobs=1,
            oob_score=False, random_state=1, verbose=0, warm_start=False)

#So, the optimized value for the max_depth variable is 3 and for n_estimator is 41. 
#Now letÅfs build the model using these optimized values.

i=1
kf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)
for train_index,test_index in kf.split(X,y):
     print('\n{} of kfold {}'.format(i,kf.n_splits))
     xtr,xvl = X.loc[train_index],X.loc[test_index]
     ytr,yvl = y[train_index],y[test_index]
    
     model = RandomForestClassifier(random_state=1, max_depth=3, n_estimators=41)
     model.fit(xtr, ytr)
     pred_test = model.predict(xvl)
     score = accuracy_score(yvl,pred_test)
     print('accuracy_score',score)
     i+=1
pred_test = model.predict(test)
pred2=model.predict_proba(test)[:,1]

submission['Loan_Status']=pred_test            # filling Loan_Status with predictions
submission['Loan_ID']=test_original['Loan_ID'] # filling Loan_ID with test Loan_ID
# replacing 0 and 1 with N and Y
submission['Loan_Status'].replace(0, 'N',inplace=True)
submission['Loan_Status'].replace(1, 'Y',inplace=True)
# Converting submission file to .csv format
pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Random Forest.csv')

#We got an accuracy of 0.7638 from the random forest model on leaderboard.

#Let us find the feature importance now, i.e. which features are most important for this #problem. 
#We will use feature_importances_ attribute of sklearn to do so.

importances=pd.Series(model.feature_importances_, index=X.columns)
importances.plot(kind='barh', figsize=(12,8))

#We can see that Credit_History is the most important feature followed by Balance Income, #Total Income, EMI. 
#So, feature engineering helped us in predicting our target variable.

#XGBoost

#XGBoost works only with numeric variables and we have already replaced the categorical variables
#with numeric variables. LetÅfs have a look at the parameters that we are going to use in our model.

n_estimator: This specifies the number of trees for the model.
max_depth: We can specify maximum depth of a tree using this parameter.
from xgboost import XGBClassifier

i=1
kf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)
for train_index,test_index in kf.split(X,y):
     print('\n{} of kfold {}'.format(i,kf.n_splits))
     xtr,xvl = X.loc[train_index],X.loc[test_index]
     ytr,yvl = y[train_index],y[test_index]
    
     model = XGBClassifier(n_estimators=50, max_depth=4)
     model.fit(xtr, ytr)
     pred_test = model.predict(xvl)
     score = accuracy_score(yvl,pred_test)
     print('accuracy_score',score)
     i+=1
pred_test = model.predict(test)
pred3=model.predict_proba(test)[:,1]

#The mean validation accuracy for this model is 0.79

submission['Loan_Status']=pred_test
submission['Loan_ID']=test_original['Loan_ID']
submission['Loan_Status'].replace(0, 'N',inplace=True)
submission['Loan_Status'].replace(1, 'Y',inplace=True)
pd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('XGBoost.csv')

#We got an accuracy of 0.73611 with this model.

#After trying and testing 4 different algorithms, the best accuracy on the public leaderboard is 
#achieved by Logistic Regression (0.7847), followed by RandomForest (0.7638).


## Other things to explore
#What more can be tried?
#There are still quite a many things that can be tried to improve our modelsÅf predictions. 
#We create and add more variables, try different models with different subset of features and/or rows, etc.
#Some of the ideas are listed below:

#We can train the XGBoost model using grid search to optimize its hyperparameters and improve the accuracy.
#We can combine the applicants with 1,2,3 or more dependents and make a new feature as discussed in the EDA part.
#We can also make independent vs independent variable visualizations to discover some more patterns.
#We can also arrive at the EMI using a better formula which may include interest rates as well.